{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ1bnqrNSOQoRnfhHLr4jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoTarricone/PAI_task_1/blob/main/Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import typing\n",
        "from sklearn.gaussian_process.kernels import *\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "##Extra libraries\n",
        "import random\n",
        "\n",
        "\n",
        "# Set `EXTENDED_EVALUATION` to `True` in order to visualize your predictions.\n",
        "EXTENDED_EVALUATION = False\n",
        "EVALUATION_GRID_POINTS = 300  # Number of grid points used in extended evaluation\n",
        "\n",
        "# Cost function constants\n",
        "COST_W_UNDERPREDICT = 50.0\n",
        "COST_W_NORMAL = 1.0\n"
      ],
      "metadata": {
        "id": "WBgNRL9PTurp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zrS9VOP_67lT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Model(object):\n",
        "    \"\"\"\n",
        "    Model for this task.\n",
        "    You need to implement the fit_model and predict methods\n",
        "    without changing their signatures, but are allowed to create additional methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel = RBF(1.0)):\n",
        "        \"\"\"\n",
        "        Initialize your model here.\n",
        "        We already provide a random number generator for reproducibility.\n",
        "        \"\"\"\n",
        "        self.rng = np.random.default_rng(seed=0)\n",
        "        self.model = GaussianProcessRegressor(kernel = kernel)\n",
        "\n",
        "        # TODO: Add custom initialization for your model here if necessary\n",
        "\n",
        "    def make_predictions(self, test_x_2D: np.ndarray, test_x_AREA: np.ndarray) -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Predict the pollution concentration for a given set of city_areas.\n",
        "        :param test_x_2D: city_areas as a 2d NumPy float array of shape (NUM_SAMPLES, 2)\n",
        "        :param test_x_AREA: city_area info for every sample in a form of a bool array (NUM_SAMPLES,)\n",
        "        :return:\n",
        "            Tuple of three 1d NumPy float arrays, each of shape (NUM_SAMPLES,),\n",
        "            containing your predictions, the GP posterior mean, and the GP posterior stddev (in that order)\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Use your GP to estimate the posterior mean and stddev for each city_area here\n",
        "        gp_mean = np.zeros(test_x_2D.shape[0], dtype=float)\n",
        "        gp_std = np.zeros(test_x_2D.shape[0], dtype=float)\n",
        "\n",
        "        # TODO: Use the GP posterior to form your predictions here\n",
        "        predictions = gp_mean\n",
        "\n",
        "        return predictions, gp_mean, gp_std\n",
        "\n",
        "    def fitting_model(self, train_y: np.ndarray,train_x_2D: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit your model on the given training data.\n",
        "        :param train_x_2D: Training features as a 2d NumPy float array of shape (NUM_SAMPLES, 2)\n",
        "        :param train_y: Training pollution concentrations as a 1d NumPy float array of shape (NUM_SAMPLES,)\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Fit your model here\n",
        "        self.model.fit(train_x_2D, train_y)\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You don't have to change this function\n",
        "def cost_function(ground_truth: np.ndarray, predictions: np.ndarray, AREA_idxs: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the cost of a set of predictions.\n",
        "\n",
        "    :param ground_truth: Ground truth pollution levels as a 1d NumPy float array\n",
        "    :param predictions: Predicted pollution levels as a 1d NumPy float array\n",
        "    :param AREA_idxs: city_area info for every sample in a form of a bool array (NUM_SAMPLES,)\n",
        "    :return: Total cost of all predictions as a single float\n",
        "    \"\"\"\n",
        "    assert ground_truth.ndim == 1 and predictions.ndim == 1 and ground_truth.shape == predictions.shape\n",
        "\n",
        "    # Unweighted cost\n",
        "    cost = (ground_truth - predictions) ** 2\n",
        "    weights = np.ones_like(cost) * COST_W_NORMAL\n",
        "\n",
        "    # Case i): underprediction\n",
        "    mask = (predictions < ground_truth) & [bool(AREA_idx) for AREA_idx in AREA_idxs]\n",
        "    weights[mask] = COST_W_UNDERPREDICT\n",
        "\n",
        "    # Weigh the cost and return the average\n",
        "    return np.mean(cost * weights)\n",
        "\n",
        "\n",
        "# You don't have to change this function\n",
        "def is_in_circle(coor, circle_coor):\n",
        "    \"\"\"\n",
        "    Checks if a coordinate is inside a circle.\n",
        "    :param coor: 2D coordinate\n",
        "    :param circle_coor: 3D coordinate of the circle center and its radius\n",
        "    :return: True if the coordinate is inside the circle, False otherwise\n",
        "    \"\"\"\n",
        "    return (coor[0] - circle_coor[0])**2 + (coor[1] - circle_coor[1])**2 < circle_coor[2]**2\n",
        "\n",
        "# You don't have to change this function\n",
        "def determine_city_area_idx(visualization_xs_2D):\n",
        "    \"\"\"\n",
        "    Determines the city_area index for each coordinate in the visualization grid.\n",
        "    :param visualization_xs_2D: 2D coordinates of the visualization grid\n",
        "    :return: 1D array of city_area indexes\n",
        "    \"\"\"\n",
        "    # Circles coordinates\n",
        "    circles = np.array([[0.5488135, 0.71518937, 0.17167342],\n",
        "                    [0.79915856, 0.46147936, 0.1567626 ],\n",
        "                    [0.26455561, 0.77423369, 0.10298338],\n",
        "                    [0.6976312,  0.06022547, 0.04015634],\n",
        "                    [0.31542835, 0.36371077, 0.17985623],\n",
        "                    [0.15896958, 0.11037514, 0.07244247],\n",
        "                    [0.82099323, 0.09710128, 0.08136552],\n",
        "                    [0.41426299, 0.0641475,  0.04442035],\n",
        "                    [0.09394051, 0.5759465,  0.08729856],\n",
        "                    [0.84640867, 0.69947928, 0.04568374],\n",
        "                    [0.23789282, 0.934214,   0.04039037],\n",
        "                    [0.82076712, 0.90884372, 0.07434012],\n",
        "                    [0.09961493, 0.94530153, 0.04755969],\n",
        "                    [0.88172021, 0.2724369,  0.04483477],\n",
        "                    [0.9425836,  0.6339977,  0.04979664]])\n",
        "\n",
        "    visualization_xs_AREA = np.zeros((visualization_xs_2D.shape[0],))\n",
        "\n",
        "    for i,coor in enumerate(visualization_xs_2D):\n",
        "        visualization_xs_AREA[i] = any([is_in_circle(coor, circ) for circ in circles])\n",
        "\n",
        "    return visualization_xs_AREA\n",
        "\n",
        "# # You don't have to change this function\n",
        "# def perform_extended_evaluation(model: Model, output_dir: str = '/results'):\n",
        "#     \"\"\"\n",
        "#     Visualizes the predictions of a fitted model.\n",
        "#     :param model: Fitted model to be visualized\n",
        "#     :param output_dir: Directory in which the visualizations will be stored\n",
        "#     \"\"\"\n",
        "#     print('Performing extended evaluation')\n",
        "\n",
        "#     # Visualize on a uniform grid over the entire coordinate system\n",
        "#     grid_lat, grid_lon = np.meshgrid(\n",
        "#         np.linspace(0, EVALUATION_GRID_POINTS - 1, num=EVALUATION_GRID_POINTS) / EVALUATION_GRID_POINTS,\n",
        "#         np.linspace(0, EVALUATION_GRID_POINTS - 1, num=EVALUATION_GRID_POINTS) / EVALUATION_GRID_POINTS,\n",
        "#     )\n",
        "#     visualization_xs_2D = np.stack((grid_lon.flatten(), grid_lat.flatten()), axis=1)\n",
        "#     visualization_xs_AREA = determine_city_area_idx(visualization_xs_2D)\n",
        "\n",
        "#     # Obtain predictions, means, and stddevs over the entire map\n",
        "#     predictions, gp_mean, gp_stddev = model.make_predictions(visualization_xs_2D, visualization_xs_AREA)\n",
        "#     predictions = np.reshape(predictions, (EVALUATION_GRID_POINTS, EVALUATION_GRID_POINTS))\n",
        "#     gp_mean = np.reshape(gp_mean, (EVALUATION_GRID_POINTS, EVALUATION_GRID_POINTS))\n",
        "\n",
        "#     vmin, vmax = 0.0, 65.0\n",
        "\n",
        "#     # Plot the actual predictions\n",
        "#     fig, ax = plt.subplots()\n",
        "#     ax.set_title('Extended visualization of task 1')\n",
        "#     im = ax.imshow(predictions, vmin=vmin, vmax=vmax)\n",
        "#     cbar = fig.colorbar(im, ax = ax)\n",
        "\n",
        "#     # Save figure to pdf\n",
        "#     figure_path = os.path.join(output_dir, 'extended_evaluation.pdf')\n",
        "#     fig.savefig(figure_path)\n",
        "#     print(f'Saved extended evaluation to {figure_path}')\n",
        "\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "def extract_city_area_information(train_x: np.ndarray, test_x: np.ndarray) -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extracts the city_area information from the training and test features.\n",
        "    :param train_x: Training features\n",
        "    :param test_x: Test features\n",
        "    :return: Tuple of (training features' 2D coordinates, training features' city_area information,\n",
        "        test features' 2D coordinates, test features' city_area information)\n",
        "    \"\"\"\n",
        "    train_x_2D = np.zeros((train_x.shape[0], 2), dtype=float)\n",
        "    train_x_AREA = np.zeros((train_x.shape[0],), dtype=bool)\n",
        "    test_x_2D = np.zeros((test_x.shape[0], 2), dtype=float)\n",
        "    test_x_AREA = np.zeros((test_x.shape[0],), dtype=bool)\n",
        "\n",
        "    #TODO: Extract the city_area information from the training and test features\n",
        "    train_x_2D = train_x[:,:2]\n",
        "    train_x_AREA = train_x[:,2]\n",
        "\n",
        "    test_x_2D = test_x[:,:2]\n",
        "    test_x_AREA = test_x[:,2]\n",
        "\n",
        "\n",
        "    assert train_x_2D.shape[0] == train_x_AREA.shape[0] and test_x_2D.shape[0] == test_x_AREA.shape[0]\n",
        "    assert train_x_2D.shape[1] == 2 and test_x_2D.shape[1] == 2\n",
        "    assert train_x_AREA.ndim == 1 and test_x_AREA.ndim == 1\n",
        "\n",
        "    return train_x_2D, train_x_AREA, test_x_2D, test_x_AREA\n",
        "\n"
      ],
      "metadata": {
        "id": "2tO98w2xUArU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you don't have to change this function\n",
        "def main():\n",
        "    # Load the training dateset and test features\n",
        "    train_x = np.loadtxt('train_x.csv', delimiter=',', skiprows=1)\n",
        "    train_y = np.loadtxt('train_y.csv', delimiter=',', skiprows=1)\n",
        "    test_x = np.loadtxt('test_x.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "    # Extract the city_area information\n",
        "    train_x_2D, train_x_AREA, test_x_2D, test_x_AREA = extract_city_area_information(train_x, test_x)\n",
        "    # Fit the model\n",
        "    print('Fitting model')\n",
        "    model = Model()\n",
        "    model.fitting_model(train_y,train_x_2D)\n",
        "\n",
        "    # Predict on the test features\n",
        "    print('Predicting on test features')\n",
        "    predictions = model.make_predictions(test_x_2D, test_x_AREA)\n",
        "    print(predictions)\n",
        "\n",
        "    if EXTENDED_EVALUATION:\n",
        "        perform_extended_evaluation(model, output_dir='.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "tSA-s-z9T149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "ca843fe9-0268-4f8d-aa44-2d93b2cda4d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-74005d7e56a7>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-74005d7e56a7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Load the training dateset and test features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_x.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_y.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_x.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1339\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: train_x.csv not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVYoK4epT-zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you don't have to change this function\n",
        "def main():\n",
        "    # Load the training dateset and test features\n",
        "    train_x = np.loadtxt('train_x.csv', delimiter=',', skiprows=1)\n",
        "    train_y = np.loadtxt('train_y.csv', delimiter=',', skiprows=1)\n",
        "    test_x = np.loadtxt('test_x.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "\n",
        "    # Extract the city_area information\n",
        "    train_x_2D, train_x_AREA, test_x_2D, test_x_AREA = extract_city_area_information(train_x, test_x)\n",
        "    # Fit the model\n",
        "    print('Fitting model')\n",
        "    model = Model()\n",
        "    model.fitting_model(train_y,train_x_2D)\n",
        "\n",
        "    # Predict on the test features\n",
        "    print('Predicting on test features')\n",
        "    predictions = model.make_predictions(test_x_2D, test_x_AREA)\n",
        "    print(predictions)\n",
        "\n",
        "    if EXTENDED_EVALUATION:\n",
        "        perform_extended_evaluation(model, output_dir='.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xNKUKQqFV1e",
        "outputId": "10b48518-2d05-4dc7-c71b-990650c139af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.8575  0.68625]\n",
            " [0.41125 0.675  ]\n",
            " [0.8625  0.90625]\n",
            " ...\n",
            " [0.135   0.64   ]\n",
            " [0.07125 0.78875]\n",
            " [0.31625 0.2125 ]]\n",
            "Fitting model\n",
            "Predicting on test features\n",
            "(array([0., 0., 0., ..., 0., 0., 0.]), array([0., 0., 0., ..., 0., 0., 0.]), array([0., 0., 0., ..., 0., 0., 0.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dateset and test features\n",
        "train_x = np.loadtxt('train_x.csv', delimiter=',', skiprows=1)\n",
        "train_y = np.loadtxt('train_y.csv', delimiter=',', skiprows=1)\n",
        "test_x = np.loadtxt('test_x.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "\n",
        "# Extract the city_area information\n",
        "train_x_2D, train_x_AREA, test_x_2D, test_x_AREA = extract_city_area_information(train_x, test_x)"
      ],
      "metadata": {
        "id": "U7fQsJHOTlhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_2D.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nPZ6kqTUKBL",
        "outputId": "9c9c9b3f-6a51-4d45-8d40-0d863f663cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15189, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(train_array, test_array, perc):\n",
        "  assert train_array.shape[0] == test_array.shape[0]\n",
        "\n",
        "  len = train_array.shape[0]\n",
        "  n_samples = round(len * perc / 100)\n",
        "\n",
        "  lista = [random.randrange(0,len) for i in range(n_samples)]\n",
        "\n",
        "  return train_array[lista, :], test_array[lista]\n"
      ],
      "metadata": {
        "id": "y_uGzlQwVKW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subsample(train_x_2D, train_y, 20)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrjbnIDsaXnK",
        "outputId": "a3ed27f3-9269-4b49-fbfc-f47e850a6c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3038, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}